{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hOVgrR9rMVll",
        "outputId": "6bf06312-c153-416f-f08d-8df01ea4e00d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Connecting to security.\u001b[0m\r                                                                               \rHit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "41 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "curl is already the newest version (7.81.0-1ubuntu1.20).\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.15).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Error: ollama server not responding - could not connect to ollama server, run 'ollama serve' to start it\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.46.1)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.9)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.46.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "!apt update && apt install -y curl git\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!ollama pull llama3.2\n",
        "\n",
        "!pkill -f ngrok  # Kills all ngrok processes\n",
        "!fuser -k 8501/tcp  # Kills any process using port 6000 (or use 5000 if needed)\n",
        "# Install required libraries\n",
        "OLLAMA_API_URL = \"http://127.0.0.1:11434\"\n",
        "!nohup ollama serve > /dev/null 2>&1 &\n",
        "\n",
        "!pip install streamlit PyPDF2 requests python-docx\n",
        "!pip install pyngrok\n",
        "from pyngrok import ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akm8WaQq72GN",
        "outputId": "19ebdab9-5da7-40e6-e585-73ef82d02343"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ngrok Auth Token Set!\n",
            "ğŸš€ Streamlit App is running at: https://48644e9d337d.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "# Replace with your ngrok auth token\n",
        "NGROK_AUTH_TOKEN = \"2tqZZ0zm7U5azTMJ6B7In96Eia0_7M44dRZx1KKTz8Dfd7R4Z\"\n",
        "\n",
        "# Set authentication token\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "print(\"âœ… ngrok Auth Token Set!\")\n",
        "\n",
        "public_url = ngrok.connect(8501).public_url\n",
        "print(f\"ğŸš€ Streamlit App is running at: {public_url}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXEOgJqO_Tv0",
        "outputId": "88d4bafd-ce5e-4b3d-c39d-dd489a72f953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âŒ Error from Ollama: 404, {\"error\":\"model 'llama3.2' not found\"}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "OLLAMA_API_URL = \"http://127.0.0.1:11434/api/generate\"\n",
        "\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "payload = {\n",
        "    \"model\": \"llama3.2\",\n",
        "    \"prompt\": \"What is the capital of France?\",\n",
        "    \"stream\": False\n",
        "}\n",
        "\n",
        "response = requests.post(OLLAMA_API_URL, json=payload, headers=headers)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    print(\"âœ… Response from Ollama:\", response.json()[\"response\"])\n",
        "else:\n",
        "    print(f\"âŒ Error from Ollama: {response.status_code}, {response.text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcP4T4YOVEXn",
        "outputId": "eea9cc7e-0755-4f37-ac49-8c479701d7a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import requests\n",
        "import tempfile\n",
        "import os\n",
        "import tiktoken\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import random\n",
        "import seaborn as sns\n",
        "\n",
        "OLLAMA_URL = \"http://127.0.0.1:11434/api/generate\"\n",
        "\n",
        "# Streamlit UI setup\n",
        "st.set_page_config(page_title=\"Chat with Text File (Llama 3.2)\", layout=\"wide\")\n",
        "st.title(\"ğŸ“„ Chat with Your Text File (Llama 3.2)\")\n",
        "st.caption(\"Upload a text file and get automatic chat statistics!\")\n",
        "\n",
        "# Upload the file\n",
        "uploaded_file = st.file_uploader(\"ğŸ“‚ Upload your text file\", type=[\"txt\"])\n",
        "\n",
        "if uploaded_file:\n",
        "    st.success(f\"âœ… Uploaded: {uploaded_file.name}\")\n",
        "\n",
        "    # Save the uploaded file temporarily\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".txt\") as temp_file:\n",
        "        temp_file.write(uploaded_file.read())\n",
        "        temp_txt_path = temp_file.name\n",
        "\n",
        "    # Read the text file\n",
        "    with open(temp_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        file_text = f.read()\n",
        "\n",
        "    # Tokenize and display text information\n",
        "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    tokens = enc.encode(file_text)\n",
        "    st.write(f\"Document length: {len(file_text)} characters.\")\n",
        "    st.write(f\"Number of tokens: {len(tokens)}\")\n",
        "\n",
        "    # Remove the temporary file\n",
        "    os.remove(temp_txt_path)\n",
        "\n",
        "    # Display the text (first 1000 characters for preview)\n",
        "    with st.expander(\"ğŸ” View Extracted Text\"):\n",
        "        st.text_area(\"Text Content\", file_text[:1000] + \"...\" if len(file_text) > 1000 else file_text, height=200)\n",
        "\n",
        "    def parse_chat(file_text):\n",
        "      messages = []\n",
        "      senders = set()  # To collect unique senders\n",
        "      lines = file_text.split('\\n')\n",
        "\n",
        "      for line in lines:\n",
        "        # Adjust regex to account for potential non-breaking spaces, emojis, and other special characters\n",
        "        match = re.match(r\"(\\d{1,2}/\\d{1,2}/\\d{2,4}),\\s*(\\d{1,2}:\\d{2}\\s*(AM|PM))\\s*-\\s*(.*?):\\s*(.*)\", line)\n",
        "\n",
        "        if match:\n",
        "          date = match.group(1)\n",
        "          time = match.group(2)\n",
        "          sender = match.group(4)\n",
        "          message = match.group(5)\n",
        "          timestamp = f\"{date} {time}\"\n",
        "          messages.append({\"timestamp\": timestamp, \"sender\": sender, \"message\": message})\n",
        "          senders.add(sender)\n",
        "        else:\n",
        "          # Debugging: Display the line that is being skipped\n",
        "          print(f\"Skipping line: {line}\")\n",
        "\n",
        "      return messages, list(senders)\n",
        "\n",
        "\n",
        "    # Get the parsed messages and participant names\n",
        "    messages, senders = parse_chat(file_text)\n",
        "\n",
        "    # Create a DataFrame from the messages\n",
        "    df = pd.DataFrame(messages)\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['day_of_week'] = df['timestamp'].dt.day_name()\n",
        "\n",
        "    # Refined system prompt for Llama\n",
        "    system_prompt = (\n",
        "        \"You are an AI assistant that answers questions based on the given document.\"\n",
        "        \"Give direct answers, provide numbers or percentages for statistics.\"\n",
        "        \"Here are some questions you need to answer based on the chat history:\\n\"\n",
        "        \"1. Give me the statistics data for the weekdays from the chat.\\n\"\n",
        "        \"2. Based on the timestamp, check for the immediate responses in the chat.\\n\"\n",
        "        \"3. Who uses more emojis in the chat?\\n\"\n",
        "        \"4. Analyze the entire chat history from the first recorded message to the last recorded message, without limiting the timeframe.\"\n",
        "        \"Identify the person who initiated the most conversations and provide a detailed breakdown, including the count of conversations started by each participant.\"\n",
        "        \"What is the average response time during conversations?\\n\"\n",
        "        \"5. Give me statistics of who focuses a lot on the chat?\\n\"\n",
        "        \"6. What is the overall count of the total number of messages sent by each participant?\\n\"\n",
        "        \"7. Top 3 most used words by each participant in the chat?\\n\"\n",
        "        \"8. How many times did each participant start the conversation?\\n\"\n",
        "        \"9. Who is likely to end the conversation first? Give me the count.\\n\"\n",
        "        \"10. Statistics of who used the emojis more in total?\\n\"\n",
        "        \"11. What is the total number of questions asked by each participant?\\n\"\n",
        "        \"Answer all the above questions based on the chat data provided. Each answer should be clear and concise.\"\n",
        "        \"You should also do sentiment analysis of the document uploaded, analyze emotions like 'sad','happy','anger','love','jealousy','care','disagreements','bored','funny','excitement' and other emotions.\"\n",
        "    )\n",
        "\n",
        "    # Max tokens for Llama 3.2 is 128,000 tokens\n",
        "    max_tokens = 128000\n",
        "    max_chars = max_tokens * 4\n",
        "\n",
        "    # Split text into chunks of 5000 characters (or a reasonable limit)\n",
        "    chunk_size = 5000\n",
        "    chunks = [file_text[i:i + chunk_size] for i in range(0, len(file_text), chunk_size)]\n",
        "\n",
        "    responses = []\n",
        "    for chunk in chunks:\n",
        "        ollama_payload = {\n",
        "            \"model\": \"llama3.2\",\n",
        "            \"prompt\": f\"{system_prompt}\\n\\nDocument:\\n{chunk}\\n\\nAI Answer:\",\n",
        "            \"stream\": False\n",
        "        }\n",
        "\n",
        "        response = requests.post(OLLAMA_URL, json=ollama_payload)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            result = response.json().get(\"response\", \"âš ï¸ No response from Analyzer.\")\n",
        "            responses.append(result)\n",
        "        else:\n",
        "            st.error(\"âš ï¸ Failed to get response from Analyzer. Is Ollama running?\")\n",
        "            break  # Stop further processing if thereâ€™s an error\n",
        "\n",
        "    # Combine all responses\n",
        "    final_response = \"\\n\\n\".join(responses) if responses else \"âš ï¸ No response from Analyzer.\"\n",
        "\n",
        "    # Replace placeholders with actual names dynamically\n",
        "    for sender in senders:\n",
        "        final_response = final_response.replace(\"participant\", sender)\n",
        "\n",
        "    # Display the answers\n",
        "    st.subheader(\"ğŸ“Œ Automatic Chat Analysis Results:\")\n",
        "    st.write(final_response)\n",
        "\n",
        "\n",
        "\n",
        "    #emoji usage count calculatiion\n",
        "    emoji_usage = df['message'].apply(lambda x: sum([1 for c in x if c in \"ğŸ™‚ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ˜‚ğŸ¤£ğŸ˜ŠğŸ˜ğŸ˜ğŸ¥ºğŸ™„ğŸ˜›ğŸ˜œğŸ¤”ğŸ˜³ğŸ‘€ğŸ™„ğŸ˜‘ğŸ˜ğŸ˜¬ğŸ‘ğŸ˜ŒğŸ™‚\"]))  # Adjust emoji detection logic\n",
        "    emoji_count = emoji_usage.sum()\n",
        "\n",
        "    #response time calculation\n",
        "    df['response_time'] = df['timestamp'].diff().shift(-1)  # Calculate time between messages\n",
        "    avg_response_time = df['response_time'].mean()\n",
        "\n",
        "    #calculation of weekly activity activity chat\n",
        "    def get_time_slot(hour):\n",
        "      if 5 <= hour < 8:\n",
        "        return \"Early morn.\"\n",
        "      elif 8 <= hour < 12:\n",
        "        return \"Morning\"\n",
        "      elif 12 <= hour < 17:\n",
        "        return \"Afternoon\"\n",
        "      elif 17 <= hour < 21:\n",
        "        return \"Evening\"\n",
        "      else:\n",
        "        return \"Night\"\n",
        "\n",
        "    df['hour']=df['timestamp'].dt.hour\n",
        "    df['weekday']=df['timestamp'].dt.day_name().str[:3]\n",
        "    df['time_slot']=df['hour'].apply(get_time_slot)\n",
        "\n",
        "    activity_matrix = df.groupby(['time_slot', 'weekday']).size().unstack(fill_value=0)\n",
        "\n",
        "    # Step 4: Order rows and columns\n",
        "    slot_order = [\"Early morn.\", \"Morning\", \"Afternoon\", \"Evening\", \"Late night\"]\n",
        "    day_order = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
        "    activity_matrix = activity_matrix.reindex(index=slot_order, columns=day_order)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Dashboard Title\n",
        "    st.markdown(\"## ğŸ“Š Chat Dashboard\")\n",
        "\n",
        "    # Row 1: Key Stats\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    with col1:\n",
        "      st.metric(\"ğŸ“ˆTotal Messages\", len(df))\n",
        "    with col2:\n",
        "      st.metric(\"ğŸ˜„Total Emojis\", emoji_count)\n",
        "    with col3:\n",
        "      st.metric(\"â³Avg. Response Time\", str(avg_response_time).split('.')[0])\n",
        "\n",
        "    # Row 2: Bar + Focus Pie\n",
        "    col1, col2 = st.columns([2, 1])\n",
        "    with col1:\n",
        "      st.markdown(\"#### ğŸ“… Messages by Day\")\n",
        "      day_stats = df['day_of_week'].value_counts()\n",
        "      st.write(day_stats)\n",
        "      fig, ax = plt.subplots(figsize=(3, 2))\n",
        "      day_stats.plot(kind='bar', color='skyblue', ax=ax)\n",
        "      ax.set_title('Messages Sent by Day of the Week', fontsize=10)\n",
        "      ax.set_xlabel('Day of the Week',fontsize=8)\n",
        "      ax.set_ylabel('Number of Messages',fontsize=8)\n",
        "      ax.tick_params(axis='x',labelrotation=45,labelsize=7)\n",
        "      ax.tick_params(axis='y',labelsize=7)\n",
        "      plt.tight_layout(pad=0.5)\n",
        "      st.pyplot(fig)\n",
        "\n",
        "    with col2:\n",
        "      st.markdown(\"#### ğŸ¯ Focus Pie\")\n",
        "      focus_counts=df['sender'].value_counts()\n",
        "      st.write(focus_counts)\n",
        "      def small_autopct(pct):\n",
        "        return f'{pct:.1f}%' if pct > 0 else ''\n",
        "      fig, ax = plt.subplots(figsize=(1.8, 1.8))\n",
        "      focus_counts.plot(kind='pie', ax=ax, autopct=small_autopct,colors=['#b38eda','#8ec5da'], textprops={'fontsize': 5})\n",
        "      ax.set_ylabel('')\n",
        "      ax.set_title('', fontsize=7)\n",
        "      ax.set_position([0.1, 0.1, 0.8, 0.8])\n",
        "      st.pyplot(fig)\n",
        "\n",
        "    # Row 3: Sentiment Pie\n",
        "    col1,col2=st.columns([1,2])\n",
        "    with col1:\n",
        "      st.markdown(\"#### ğŸ‘» Sentiment Breakdown\")\n",
        "      sentiments = ['happy', 'sad', 'angry', 'excited', 'bored','funny','love','care','disagreements']  # Placeholder, you can replace with actual sentiment analysis\n",
        "      sentiment_counts = Counter([random.choice(sentiments) for _ in range(len(df))])\n",
        "      st.write(sentiment_counts)\n",
        "\n",
        "      fig, ax = plt.subplots(figsize=(2, 2))\n",
        "      sentiment_counts=Counter([random.choice(sentiments) for _ in range(len(df))])\n",
        "      sentiment_series=pd.Series(sentiment_counts)\n",
        "      def small_autopct(pct):\n",
        "        return f'{pct:.1f}%' if pct > 0 else ''\n",
        "      sentiment_series.plot(kind='pie', ax=ax, autopct=small_autopct,colors=['#ff9999','#66b3ff','#99ff99','#ffcc99','#21aed8','#c292ef','#ef92c1','#b97e7e','#d1e377'],textprops={'fontsize': 5})\n",
        "      ax.set_ylabel('')\n",
        "      ax.set_title('sentiments', fontsize=7)\n",
        "      ax.set_position([0.1, 0.1, 0.8, 0.8])\n",
        "      st.pyplot(fig)\n",
        "\n",
        "    with col2:\n",
        "      st.markdown(\"#### â˜€ï¸ğŸŒ” weekly chat activity\")\n",
        "      fig,ax=plt.subplots(figsize=(7,3.5))\n",
        "      sns.heatmap(activity_matrix, annot=True, fmt='.0f', cmap='YlOrBr', cbar=False, linewidths=0.5, linecolor='white', ax=ax)\n",
        "      ax.set_xlabel(\"day of the week\",fontsize=10)\n",
        "      ax.set_ylabel(\"time of the day\",fontsize=10)\n",
        "      ax.set_title(\"chat data by time and day\",fontsize=10,pad=10)\n",
        "\n",
        "      ax.tick_params(axis='x',labelrotation=0,labelsize=8)\n",
        "      ax.tick_params(axis='y',labelsize=8)\n",
        "      st.pyplot(fig)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # User's Question input\n",
        "    question = st.text_input(\"ğŸ’¡ Ask a question about the document\")\n",
        "\n",
        "    if question:\n",
        "        st.info(\"ğŸ¤– Sending question to Llama 3.2 :)...\")\n",
        "\n",
        "        # Refined system prompt to focus on direct answers only\n",
        "        system_prompt_user = (\n",
        "            \"You are an AI assistant that answers questions based on the given document.\"\n",
        "            \"When asked about a specific message, return only the exact date and time when the message was sent.\"\n",
        "            \"If the message is not found or timestamped, respond with 'I don't know'.\"\n",
        "            \"Do not provide any additional context, reasoning, or elaboration.\"\n",
        "            \"Only return the direct date and time of the message. Do not include any other information.\"\n",
        "            \"Perform sentiment analysis on the document uploaded, analyze the emotions such as 'upset','anger','jealous','care','love','emotional','happiness','sad','funny', and others.\"\n",
        "            \"Provide accurate answers. Don't make things up.\"\n",
        "            \"If the question is not related to the document, politely respond that you are tuned only to answer questions that are related to the document.\"\n",
        "        )\n",
        "\n",
        "        # Max tokens for Llama 3.2 is 128,000 tokens\n",
        "        max_tokens = 128000\n",
        "        chunk_size = 5000  # Split text into chunks of 5000 characters (or a reasonable limit)\n",
        "        chunks = [file_text[i:i + chunk_size] for i in range(0, len(file_text), chunk_size)]\n",
        "\n",
        "        responses_user_question = []\n",
        "        for chunk in chunks:\n",
        "            ollama_payload_user_question = {\n",
        "                \"model\": \"llama3.2\",\n",
        "                \"prompt\": f\"{system_prompt_user}\\n\\nDocument:\\n{chunk}\\n\\nUser Question: {question}\\n\\nAI Answer:\",\n",
        "                \"stream\": False\n",
        "            }\n",
        "\n",
        "            response = requests.post(OLLAMA_URL, json=ollama_payload_user_question)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                result = response.json().get(\"response\", \"âš ï¸ No response from Analyzer.\")\n",
        "                responses_user_question.append(result)\n",
        "            else:\n",
        "                st.error(\"âš ï¸ Failed to get response from Analyzer. Is Ollama running?\")\n",
        "                break\n",
        "\n",
        "        # Combine all responses for the user's question\n",
        "        final_response_user_question = \"\\n\\n\".join(responses_user_question) if responses_user_question else \"âš ï¸ No response from Analyzer\"\n",
        "\n",
        "        # Display the user's answer\n",
        "        st.subheader(\"ğŸ“Œ User's Question Answer:\")\n",
        "        st.write(final_response_user_question)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "en2uruIHZpoA",
        "outputId": "43d29da3-119a-4e8e-caff-df00a9e87bbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.7.9)\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.169.187.222:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken\n",
        "!streamlit run app.py &"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}